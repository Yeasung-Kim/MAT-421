{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyORUwkbCFfcvwae7L8qOn9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yeasung-Kim/MAT-421/blob/main/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1.1: Introduction to Linear Algebra\n",
        "\n",
        "Linear algebra is fundamental in various applications, especially in data science and machine learning.\n",
        "It includes concepts such as vector spaces, orthogonality, eigenvalues, and matrix decomposition.\n",
        "Linear algebra is the study of vectors and matrices, which are used to represent and manipulate data efficiently.\n",
        "It plays a crucial role in solving systems of linear equations, transforming data in machine learning, and optimizing computations in various domains.\n"
      ],
      "metadata": {
        "id": "RBT9x6sfokjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1.2: Elements of Linear Algebra\n",
        "\n",
        "## Linear Combinations\n",
        "\n",
        "A linear combination involves multiplying each vector by a scalar and summing the results.\n",
        "For example, given vectors v1, v2, and scalars a1, a2, the linear combination is:\n",
        "    a1 * v1 + a2 * v2\n",
        "This concept is useful in determining if a set of vectors can span a space."
      ],
      "metadata": {
        "id": "4Z_PNhu9ouZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import eig, qr, inv\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "A = np.array([[2, 1], [1, 3]])\n",
        "b = np.array([5, 6])\n",
        "linear_combination = A @ np.array([2, 3])  # Example of a linear combination\n",
        "print(\"Linear Combination Result:\", linear_combination)\n"
      ],
      "metadata": {
        "id": "0NWFr64zo3Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Independence and Basis\n",
        "\n",
        "A set of vectors is linearly independent if none of the vectors can be written as a linear combination of the others.\n",
        "If a set of vectors is linearly dependent, at least one of the vectors can be represented as a combination of others.\n",
        "The basis of a vector space is a set of linearly independent vectors that span the entire space.\n"
      ],
      "metadata": {
        "id": "JnCVqGDtpCzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = np.array([[1, 2], [2, 4]])\n",
        "rank_V = np.linalg.matrix_rank(V)\n",
        "print(\"Rank of V:\", rank_V)"
      ],
      "metadata": {
        "id": "5Wc1KNH_pGPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigenvalues and Eigenvectors\n",
        "\n",
        "Eigenvalues and eigenvectors are useful in understanding transformations.\n",
        "An eigenvector of a matrix A is a vector v such that:\n",
        "    A * v = λ * v\n",
        "where λ is the eigenvalue corresponding to v. Eigenvalues indicate how much the transformation scales the vector."
      ],
      "metadata": {
        "id": "CD1sr_e2pINF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[4, -2], [1, 1]])\n",
        "values, vectors = eig(A)\n",
        "print(\"Eigenvalues:\", values)\n",
        "print(\"Eigenvectors:\\n\", vectors)\n"
      ],
      "metadata": {
        "id": "kLpMKbM2pLmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1.3: Linear Regression\n",
        "\n",
        "## Introduction to Linear Regression\n",
        "\n",
        "Linear regression is a statistical method used to model the relationship between a dependent variable (Y) and one or more independent variables (X).\n",
        "It is represented by the equation:\n",
        "    Y = β0 + β1 * X + ε\n",
        "where β0 is the intercept, β1 is the slope, and ε is the error term.\n",
        "The goal is to minimize the sum of squared residuals to find the best-fitting line.\n"
      ],
      "metadata": {
        "id": "Ux2s9utcpOgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Generating Sample Data\n",
        "np.random.seed(0)\n",
        "x = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * x + np.random.randn(100, 1)  # Adding noise\n",
        "\n",
        "## Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "y_pred = model.predict(x)\n",
        "\n",
        "## Plotting the Results\n",
        "plt.scatter(x, y, color='blue', label='Data')\n",
        "plt.plot(x, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend()\n",
        "plt.title(\"Linear Regression Example\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UMuNlR3vpR9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QR Decomposition for Least Squares\n",
        "\n",
        "QR decomposition is a method to factorize a matrix A into an orthogonal matrix Q and an upper triangular matrix R:\n",
        "    A = QR\n",
        "This is useful for solving least-squares problems efficiently.\n"
      ],
      "metadata": {
        "id": "azb2i_83pW9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q, R = qr(A)\n",
        "print(\"Q Matrix:\\n\", Q)\n",
        "print(\"R Matrix:\\n\", R)"
      ],
      "metadata": {
        "id": "WDQupc6gpYp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving Least Squares using QR Decomposition\n",
        "\n",
        "One application of QR decomposition is solving the least squares problem Ax = b.\n",
        "Instead of computing the inverse, we solve using the decomposition:\n",
        "    Rx = Q^T b\n",
        "which can be solved efficiently using back-substitution.\n"
      ],
      "metadata": {
        "id": "KZrLmbMdpbAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.array([1, 2])\n",
        "y = np.dot(Q.T, b)\n",
        "x_ls = np.linalg.solve(R, y)\n",
        "print(\"Least Squares Solution:\", x_ls)\n"
      ],
      "metadata": {
        "id": "Pi3nryw_pdkB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}