{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMvLfIs01z5c36lOyDeVPUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yeasung-Kim/MAT-421/blob/main/HW_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Differentiation Problem Statement\n",
        "\n",
        "Numerical differentiation is the process of estimating the derivative of a function using numerical techniques rather than analytical differentiation. This is especially useful when dealing with experimental data, complex functions, or when an explicit derivative is not available.\n",
        "\n",
        "A **numerical grid** consists of discrete points that sample the function over a defined interval. If \\( x \\) is a numerical grid with spacing \\( h \\), then the difference between two adjacent points is \\( h \\). This spacing is critical in determining the accuracy of numerical differentiation methods.\n",
        "\n",
        "In real-world applications, exact function values may not be available continuously, requiring discrete approximations of derivatives using finite difference methods.\n"
      ],
      "metadata": {
        "id": "SdacPdR5Ktjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Numerical Differentiation Problem Statement - Example Code\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function\n",
        "def f(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "# Create a numerical grid\n",
        "x = np.linspace(0, 2*np.pi, 100)\n",
        "y = f(x)\n",
        "\n",
        "# Plot function\n",
        "tl = 'Numerical Grid Representation'\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(x, y, label='$f(x) = \\sin(x)$')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title(tl)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rH9F_9OWMGEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finite Difference Approximating Derivatives\n",
        "\n",
        "Finite difference methods provide a way to approximate derivatives using function values at specific points. Three primary methods exist:\n",
        "\n",
        "1. **Forward Difference Approximation:**\n",
        "   - Uses the function value at \\( x \\) and \\( x+h \\) to estimate the derivative.\n",
        "   - Formula:\n",
        "     \\[\n",
        "     f'(x) \\approx \\frac{f(x+h) - f(x)}{h}\n",
        "     \\]\n",
        "   - Accuracy: \\( O(h) \\) (First-order accurate)\n",
        "\n",
        "2. **Backward Difference Approximation:**\n",
        "   - Uses function values at \\( x \\) and \\( x-h \\) for approximation.\n",
        "   - Formula:\n",
        "     \\[\n",
        "     f'(x) \\approx \\frac{f(x) - f(x-h)}{h}\n",
        "     \\]\n",
        "   - Accuracy: \\( O(h) \\)\n",
        "\n",
        "3. **Central Difference Approximation:**\n",
        "   - Uses function values at \\( x-h \\) and \\( x+h \\), providing a more accurate estimation.\n",
        "   - Formula:\n",
        "     \\[\n",
        "     f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\n",
        "     \\]\n",
        "   - Accuracy: \\( O(h^2) \\) (Higher accuracy compared to forward and backward differences)\n",
        "\n",
        "Central difference is generally preferred due to its better accuracy, as it cancels out lower-order error terms using symmetric points."
      ],
      "metadata": {
        "id": "hBlmk81ZLHny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Finite Difference Approximating Derivatives - Example Code\n",
        "\n",
        "h = 0.1\n",
        "x_vals = np.arange(0, 2*np.pi, h)\n",
        "y_vals = np.sin(x_vals)\n",
        "\n",
        "# Forward Difference Approximation\n",
        "forward_diff = np.diff(y_vals) / h\n",
        "x_diff = x_vals[:-1]\n",
        "\n",
        "# Exact derivative\n",
        "exact_derivative = np.cos(x_diff)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(x_diff, forward_diff, '--', label='Forward Difference')\n",
        "plt.plot(x_diff, exact_derivative, label='Exact Derivative')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Derivative')\n",
        "plt.legend()\n",
        "plt.title('Forward Difference Approximation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "skaYmANQMN4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approximating Higher Order Derivatives\n",
        "\n",
        "Finite difference methods can also approximate second and higher-order derivatives. The second derivative is particularly useful in analyzing curvature and acceleration in physical systems.\n",
        "\n",
        "The second derivative approximation using finite differences is given by:\n",
        "\\[\n",
        " f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}\n",
        "\\]\n",
        "This formula is derived by combining Taylor series expansions at \\( x+h \\) and \\( x-h \\) and eliminating the first derivative term.\n",
        "\n",
        "For higher-order derivatives, additional terms from Taylor series expansion can be included. For instance, the third derivative can be approximated as:\n",
        "\\[\n",
        " f'''(x) \\approx \\frac{f(x+2h) - 2f(x+h) + 2f(x-h) - f(x-2h)}{2h^3}\n",
        "\\]\n",
        "These approximations are widely used in physics, engineering, and scientific computing when analytical differentiation is infeasible.\n"
      ],
      "metadata": {
        "id": "VOmUnq6tLOEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Approximating Higher Order Derivatives - Example Code\n",
        "\n",
        "second_derivative = (np.sin(x_vals[:-2]) - 2*np.sin(x_vals[1:-1]) + np.sin(x_vals[2:])) / h**2\n",
        "x_second_deriv = x_vals[1:-1]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(x_second_deriv, second_derivative, '--', label='Second Derivative Approx.')\n",
        "plt.plot(x_second_deriv, -np.sin(x_second_deriv), label='Exact Second Derivative')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Second Derivative')\n",
        "plt.legend()\n",
        "plt.title('Approximating Higher Order Derivatives')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BsdhiFZ3MQnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Differentiation with Noise\n",
        "\n",
        "In practical applications, numerical differentiation is often applied to real-world data, which may contain measurement noise. Noise can significantly impact the accuracy of derivative approximations, leading to large variations in computed derivatives.\n",
        "\n",
        "To mitigate noise effects, the following techniques can be applied:\n",
        "\n",
        "- **Smoothing Filters:** Applying moving averages or Gaussian filters before differentiation can reduce noise impact.\n",
        "- **Higher-order Approximations:** Using more data points in finite difference methods can improve accuracy.\n",
        "- **Regularization Methods:** Methods like Tikhonov regularization can stabilize noisy derivative estimates.\n",
        "\n",
        "For example, in signal processing, differentiating noisy sensor data can lead to amplified errors. Applying a low-pass filter before computing derivatives helps in reducing such errors.\n",
        "\n",
        "Understanding numerical differentiation and its associated errors is crucial for applications in engineering, physics, and computational sciences, where exact derivatives are often unavailable."
      ],
      "metadata": {
        "id": "v9_BiQpCLTMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Numerical Differentiation with Noise - Example Code\n",
        "\n",
        "np.random.seed(0)\n",
        "y_noisy = np.sin(x_vals) + np.random.normal(0, 0.1, len(x_vals))\n",
        "\n",
        "# Compute noisy derivative\n",
        "noisy_derivative = np.diff(y_noisy) / h\n",
        "x_noisy_diff = x_vals[:-1]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(x_noisy_diff, noisy_derivative, '--', label='Noisy Numerical Derivative')\n",
        "plt.plot(x_noisy_diff, np.cos(x_noisy_diff), label='Exact Derivative')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Derivative')\n",
        "plt.legend()\n",
        "plt.title('Numerical Differentiation with Noise')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JDptvzTGMTBf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}